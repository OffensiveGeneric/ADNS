import os
import random
import shutil
import subprocess
from datetime import datetime, timedelta, timezone

from flask import Flask, jsonify, request
from flask_cors import CORS
from flask_sqlalchemy import SQLAlchemy
from sqlalchemy import inspect, text
from sqlalchemy.exc import SQLAlchemyError

from model_runner import DetectionEngine
from task_queue import enqueue_flow_scoring

app = Flask(__name__)
CORS(app)

DEFAULT_DB_URI = "postgresql://adns:adns_password@127.0.0.1/adns"
app.config["SQLALCHEMY_DATABASE_URI"] = os.environ.get("SQLALCHEMY_DATABASE_URI", DEFAULT_DB_URI)
app.config["SQLALCHEMY_TRACK_MODIFICATIONS"] = False

db = SQLAlchemy(app)

MAX_FLOWS = 400  # keep last N flows when responding to dashboard clients
FLOW_RETENTION_MINUTES = int(os.environ.get("ADNS_FLOW_RETENTION_MINUTES", "30"))
FLOW_RETENTION_MAX_ROWS = int(os.environ.get("ADNS_FLOW_RETENTION_MAX_ROWS", "5000"))
KILL_SWITCH_STATE = {"enabled": False}
KILL_SWITCH_INTERFACE = os.environ.get("ADNS_KILLSWITCH_INTERFACE", "eth0")
USE_NSENTER = os.environ.get("ADNS_NSENTER_HOST", "true").lower() not in {"0", "false", "no"}

PROTOCOL_MAP = {
    "1": "ICMP",
    "6": "TCP",
    "17": "UDP",
    "41": "ENCAP",
    "47": "GRE",
    "50": "ESP",
    "51": "AH",
    "58": "ICMPv6",
    "132": "SCTP",
}


class Flow(db.Model):
    __tablename__ = "flows"

    id = db.Column(db.Integer, primary_key=True)
    timestamp = db.Column(db.DateTime(timezone=True), nullable=False, index=True)
    src_ip = db.Column(db.String(64), nullable=False, index=True)
    dst_ip = db.Column(db.String(64), nullable=False, index=True)
    proto = db.Column(db.String(16), nullable=False)
    bytes = db.Column(db.Integer, nullable=False, default=0)
    extra = db.Column(db.JSON, nullable=True)

    predictions = db.relationship("Prediction", backref="flow", lazy="dynamic", cascade="all, delete-orphan")


class Prediction(db.Model):
    __tablename__ = "predictions"
    __table_args__ = (db.UniqueConstraint("flow_id", name="uq_predictions_flow_id"),)

    id = db.Column(db.Integer, primary_key=True)
    flow_id = db.Column(db.Integer, db.ForeignKey("flows.id"), nullable=False, index=True)
    score = db.Column(db.Float, nullable=True)
    label = db.Column(db.String(32), nullable=True)
    created_at = db.Column(db.DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc))


class BlockedIP(db.Model):
    __tablename__ = "blocked_ips"

    id = db.Column(db.Integer, primary_key=True)
    ip = db.Column(db.String(64), unique=True, nullable=False)
    active = db.Column(db.Boolean, nullable=False, default=True)
    created_at = db.Column(db.DateTime(timezone=True), nullable=False, default=lambda: datetime.now(timezone.utc))


def init_db() -> None:
    with app.app_context():
        db.create_all()
        ensure_flow_extra_column()
        ensure_prediction_flow_unique_index()


def ensure_flow_extra_column() -> None:
    """
    Older deployments created the flows table before `extra` existed. Ensure the
    JSON column is present so inserts from the tshark agent succeed.
    """
    try:
        inspector = inspect(db.engine)
        columns = {col["name"] for col in inspector.get_columns("flows")}
    except SQLAlchemyError as exc:  # pragma: no cover - defensive
        app.logger.warning("failed to inspect flows table: %s", exc)
        return

    if "extra" in columns:
        return

    column_type = "JSONB" if db.engine.dialect.name == "postgresql" else "JSON"
    stmt = text(f"ALTER TABLE flows ADD COLUMN extra {column_type}")
    try:
        with db.engine.begin() as conn:
            conn.execute(stmt)
        app.logger.info("added flows.extra column (%s)", column_type)
    except SQLAlchemyError as exc:  # pragma: no cover - defensive
        app.logger.error("failed to add flows.extra column: %s", exc)


def ensure_prediction_flow_unique_index() -> None:
    """Guarantee that predictions.flow_id stays unique for ON CONFLICT logic."""
    index_name = "idx_predictions_flow_id_unique"
    try:
        inspector = inspect(db.engine)
        indexes = inspector.get_indexes("predictions")
    except SQLAlchemyError as exc:  # pragma: no cover - defensive
        app.logger.warning("failed to inspect predictions indexes: %s", exc)
        return

    for idx in indexes:
        columns = idx.get("column_names") or []
        if idx.get("unique") and columns == ["flow_id"]:
            return
        if idx.get("name") == index_name and idx.get("unique"):
            return

    # Prune duplicate rows so the unique index can be created safely.
    removed = 0
    duplicates = (
        db.session.query(Prediction.flow_id)
        .group_by(Prediction.flow_id)
        .having(db.func.count(Prediction.id) > 1)
        .all()
    )
    for (flow_id,) in duplicates:
        dup_ids = (
            db.session.query(Prediction.id)
            .filter(Prediction.flow_id == flow_id)
            .order_by(Prediction.id.asc())
            .all()
        )
        ids_to_delete = [row.id for row in dup_ids[1:]]
        if ids_to_delete:
            removed += (
                Prediction.query.filter(Prediction.id.in_(ids_to_delete)).delete(synchronize_session=False)
            )

    if removed:
        db.session.commit()
        app.logger.info("pruned %d duplicate prediction row(s) before enforcing uniqueness", removed)
    else:
        db.session.rollback()

    stmt = text(
        f"CREATE UNIQUE INDEX IF NOT EXISTS {index_name} ON predictions(flow_id)"
    )
    try:
        with db.engine.begin() as conn:
            conn.execute(stmt)
    except SQLAlchemyError as exc:  # pragma: no cover - defensive
        app.logger.error("failed to create unique predictions index: %s", exc)


def _run_cmd(cmd: list[str]) -> tuple[bool, str]:
    prefixed = cmd
    if USE_NSENTER and shutil.which("nsenter"):
        prefixed = ["nsenter", "-t", "1", "-n"] + cmd
    try:
        proc = subprocess.run(prefixed, check=True, capture_output=True, text=True)
        return True, proc.stdout.strip()
    except FileNotFoundError as exc:
        app.logger.error("command not found: %s", exc)
        return False, "command not found"
    except subprocess.CalledProcessError as exc:
        stderr = (exc.stderr or "").strip()
        app.logger.error("command failed (%s): %s", prefixed, stderr)
        return False, stderr or "command failed"


def ensure_killswitch_rules_enabled(enabled: bool) -> None:
    """
    Toggle iptables DROP rules on the configured interface. Best effort;
    requires NET_ADMIN on the host/namespace where this runs.
    """
    iface = KILL_SWITCH_INTERFACE
    rules = [
        ["iptables", "-C", "OUTPUT", "-o", iface, "-j", "DROP"],
        ["iptables", "-C", "INPUT", "-i", iface, "-j", "DROP"],
    ]
    existing = []
    for rule in rules:
        ok, _ = _run_cmd(rule)
        existing.append(ok)

    if enabled:
        if not existing[0]:
            _run_cmd(["iptables", "-I", "OUTPUT", "-o", iface, "-j", "DROP"])
        if not existing[1]:
            _run_cmd(["iptables", "-I", "INPUT", "-i", iface, "-j", "DROP"])
    else:
        if existing[0]:
            _run_cmd(["iptables", "-D", "OUTPUT", "-o", iface, "-j", "DROP"])
        if existing[1]:
            _run_cmd(["iptables", "-D", "INPUT", "-i", iface, "-j", "DROP"])


def block_ip_os(ip: str, allow: bool = False) -> tuple[bool, str]:
    """
    Apply or remove a DROP rule for the given source IP. Best effort; requires NET_ADMIN.
    """
    check = ["iptables", "-C", "INPUT", "-s", ip, "-j", "DROP"]
    exists, _ = _run_cmd(check)
    if allow:
        if exists:
            return _run_cmd(["iptables", "-D", "INPUT", "-s", ip, "-j", "DROP"])
        return True, "rule absent"
    if exists:
        return True, "already blocked"
    return _run_cmd(["iptables", "-I", "INPUT", "-s", ip, "-j", "DROP"])


simulation_detector = DetectionEngine()

SIMULATION_TYPES = {
    "botnet_flood": {
        "label": "IoT botnet flood",
        "default_count": 120,
    },
    "data_exfiltration": {
        "label": "Data exfiltration burst",
        "default_count": 90,
    },
    "port_scan": {
        "label": "Stealthy port scan",
        "default_count": 160,
    },
}


def _pattern_ip(pattern: str, rng: random.Random) -> str:
    parts = pattern.split(".")
    octets = []
    for part in parts:
        if part in {"x", "*"}:
            octets.append(str(rng.randint(1, 254)))
        elif part == "y":
            octets.append(str(rng.randint(0, 99)))
        else:
            octets.append(part)
    while len(octets) < 4:
        octets.append(str(rng.randint(1, 254)))
    return ".".join(octets[:4])


def generate_attack_flows(kind: str, count: int) -> list[Flow]:
    rng = random.Random()
    now = datetime.now(timezone.utc)
    flows: list[Flow] = []

    def _make_extra(proto: str, src_port: int, dst_port: int, byte_count: int, service_hint: str | None = None):
        total = max(0, int(byte_count))
        reply = int(total * rng.uniform(0.05, 0.3))
        return {
            "src_port": src_port,
            "dst_port": dst_port,
            "service": service_hint or proto.lower(),
            "duration": rng.uniform(2.0, 15.0),
            "src_bytes": total,
            "dst_bytes": reply,
            "src_pkts": max(3, total // 600),
            "dst_pkts": max(3, reply // 600),
        }

    def _add_flow(ts_offset: float, src: str, dst: str, proto: str, byte_count: int, extra: dict | None = None) -> None:
        flow = Flow(
            timestamp=now - timedelta(seconds=ts_offset),
            src_ip=src,
            dst_ip=dst,
            proto=normalize_protocol(proto),
            bytes=max(0, int(byte_count)),
        )
        flow.extra = extra
        flows.append(flow)

    for i in range(count):
        if kind == "botnet_flood":
            dst = rng.choice(["198.51.100.42", "198.51.100.47", "203.0.113.10"])
            src = _pattern_ip("10.x.x.x", rng)
            bytes_val = rng.randint(120_000, 420_000)
            offset = rng.uniform(0, 90)
            src_port = rng.randint(1024, 65000)
            extra = _make_extra("tcp", src_port, 80, bytes_val, "http")
            _add_flow(offset, src, dst, "TCP", bytes_val, extra)
        elif kind == "data_exfiltration":
            src = rng.choice(["10.0.5.33", "10.0.5.34"])
            dst = _pattern_ip("203.0.113.x", rng)
            bytes_val = rng.randint(180_000, 450_000)
            offset = rng.uniform(0, 120)
            src_port = rng.randint(20000, 60000)
            extra = _make_extra("tcp", src_port, 443, bytes_val, "https")
            _add_flow(offset, src, dst, "TCP", bytes_val, extra)
        elif kind == "port_scan":
            src = rng.choice(["172.16.8.4", "172.16.8.5"])
            dst = f"192.168.{rng.randint(1, 10)}.{(i % 200) + 1}"
            proto = rng.choice(["UDP", "TCP"])
            bytes_val = rng.randint(800, 5000)
            offset = rng.uniform(0, 180)
            dst_port = rng.randint(1, 1024)
            src_port = rng.randint(20000, 65000)
            extra = _make_extra(proto.lower(), src_port, dst_port, bytes_val, proto.lower())
            _add_flow(offset, src, dst, proto, bytes_val, extra)
        else:
            raise ValueError(f"unsupported attack type '{kind}'")

    return flows


def parse_timestamp(value) -> datetime:
    if isinstance(value, (int, float)):
        return datetime.fromtimestamp(float(value), tz=timezone.utc)
    if isinstance(value, str):
        try:
            # allow trailing Z
            cleaned = value.replace("Z", "+00:00") if value.endswith("Z") else value
            return datetime.fromisoformat(cleaned)
        except ValueError:
            pass
    return datetime.now(timezone.utc)


def latest_prediction_score(flow: Flow) -> float:
    pred = flow.predictions.order_by(Prediction.created_at.desc()).first()
    if pred and pred.score is not None:
        return float(pred.score)
    return 0.0


def normalize_protocol(value) -> str:
    if value is None:
        return "OTHER"
    text = str(value).strip()
    if not text:
        return "OTHER"
    if text.isdigit():
        return PROTOCOL_MAP.get(text, f"PROTO_{text}")
    return text.upper()


def _coerce_int(value):
    if value is None or value == "":
        return None
    if isinstance(value, bool):
        return int(value)
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        return int(value)
    text = str(value).strip()
    if not text:
        return None
    base = 16 if text.lower().startswith("0x") else 10
    try:
        return int(text, base)
    except ValueError:
        digits = "".join(ch for ch in text if ch.isdigit())
        if digits:
            return int(digits)
    return None


def _coerce_float(value):
    if value is None or value == "":
        return None
    if isinstance(value, (int, float)):
        return float(value)
    text = str(value).strip()
    if not text:
        return None
    try:
        return float(text)
    except ValueError:
        return None


def _clean_text(value):
    if value is None:
        return None
    text = str(value).strip()
    return text or None


EXTRA_INT_FIELDS = {
    "src_port",
    "dst_port",
    "src_bytes",
    "dst_bytes",
    "src_pkts",
    "dst_pkts",
    "dns_qclass",
    "dns_qtype",
    "dns_rcode",
    "http_status_code",
    "http_request_body_len",
    "http_response_body_len",
}

EXTRA_FLOAT_FIELDS = {"duration"}

EXTRA_TEXT_FIELDS = {
    "dns_query",
    "http_method",
    "http_uri",
    "http_referrer",
    "http_version",
    "http_user_agent",
    "http_orig_mime_types",
    "http_resp_mime_types",
    "weird_name",
    "weird_addl",
    "weird_notice",
    "ssl_cipher",
}


def build_flow_extra(rec: dict) -> dict | None:
    extra: dict = {}
    for field in EXTRA_INT_FIELDS:
        val = _coerce_int(rec.get(field))
        if val is not None:
            extra[field] = val

    for field in EXTRA_FLOAT_FIELDS:
        val = _coerce_float(rec.get(field))
        if val is not None:
            extra[field] = val

    for field in EXTRA_TEXT_FIELDS:
        val = _clean_text(rec.get(field))
        if val:
            extra[field] = val

    service = _clean_text(rec.get("service"))
    if service:
        extra["service"] = service.lower()

    ssl_version = _coerce_int(rec.get("ssl_version"))
    if ssl_version is not None:
        extra["ssl_version"] = ssl_version

    # allow callers to set explicit src/dst jitter values later if desired
    return extra or None


def flow_to_dict(flow: Flow) -> dict:
    latest_label = None
    pred = flow.predictions.order_by(Prediction.created_at.desc()).first()
    score = 0.0
    if pred:
        latest_label = pred.label
        score = pred.score or 0.0

    return {
        "id": flow.id,
        "ts": flow.timestamp.isoformat(),
        "src_ip": flow.src_ip,
        "dst_ip": flow.dst_ip,
        "proto": normalize_protocol(flow.proto),
        "bytes": flow.bytes,
        "score": float(score),
        "label": latest_label,
        "extra": flow.extra or {},
    }


def is_anomalous_flow(flow: Flow) -> bool:
    pred = flow.predictions.order_by(Prediction.created_at.desc()).first()
    if not pred:
        return False
    label = (pred.label or "").lower()
    if label and label != "normal":
        return True
    return float(pred.score or 0.0) >= 0.6


def get_recent_flows(limit: int = MAX_FLOWS) -> list:
    flows = Flow.query.order_by(Flow.timestamp.desc()).limit(limit).all()
    # maintain chronological order (oldest first) for the dashboard
    return list(reversed(flows))


def enforce_flow_retention() -> int:
    purged = 0
    batch_size = 1000

    def delete_flow_batch(id_list: list[int]) -> int:
        if not id_list:
            return 0
        Prediction.query.filter(Prediction.flow_id.in_(id_list)).delete(synchronize_session=False)
        return Flow.query.filter(Flow.id.in_(id_list)).delete(synchronize_session=False)

    if FLOW_RETENTION_MINUTES > 0:
        cutoff = datetime.now(timezone.utc) - timedelta(minutes=FLOW_RETENTION_MINUTES)
        while True:
            stale_ids = (
                Flow.query.with_entities(Flow.id)
                .filter(Flow.timestamp < cutoff)
                .limit(batch_size)
                .all()
            )
            id_list = [row.id for row in stale_ids]
            if not id_list:
                break
            purged += delete_flow_batch(id_list)

    if FLOW_RETENTION_MAX_ROWS > 0:
        total = Flow.query.count()
        if total > FLOW_RETENTION_MAX_ROWS:
            excess = total - FLOW_RETENTION_MAX_ROWS
            while excess > 0:
                chunk = min(excess, batch_size)
                oldest_ids = (
                    Flow.query.order_by(Flow.timestamp.asc())
                    .with_entities(Flow.id)
                    .limit(chunk)
                    .all()
                )
                id_list = [row.id for row in oldest_ids]
                if not id_list:
                    break
                purged += delete_flow_batch(id_list)
                excess -= len(id_list)

    if purged:
        db.session.commit()
    return purged


init_db()

# ---------------------------------------------------------------
# Basic Health Check
# ---------------------------------------------------------------
@app.get("/health")
def health():
    return jsonify({"status": "ok"})


# ---------------------------------------------------------------
# Ingest endpoint for tshark agent
# ---------------------------------------------------------------
@app.route("/ingest", methods=["POST"])
def ingest():
    """
    Accepts either:
      - a single flow object
      - a list of flow objects
    and persists them in the flows table.
    """
    payload = request.get_json(force=True, silent=False)

    if isinstance(payload, dict):
        batch = [payload]
    elif isinstance(payload, list):
        batch = payload
    else:
        return jsonify({"error": "invalid payload"}), 400

    created = 0
    flow_records: list[Flow] = []
    for rec in batch:
        extra = build_flow_extra(rec)
        flow = Flow(
            timestamp=parse_timestamp(rec.get("ts")),
            src_ip=rec.get("src_ip", ""),
            dst_ip=rec.get("dst_ip", ""),
            proto=normalize_protocol(rec.get("proto", "")),
            bytes=int(rec.get("bytes") or 0),
            extra=extra,
        )
        flow_records.append(flow)
        db.session.add(flow)
        created += 1

    try:
        flow_ids: list[int] = []
        if flow_records:
            db.session.flush()
            flow_ids = [flow.id for flow in flow_records]

        db.session.commit()
    except Exception as exc:  # pragma: no cover
        db.session.rollback()
        app.logger.exception("failed to insert flows: %s", exc)
        return jsonify({"error": "database insert failed"}), 500

    purged = enforce_flow_retention()
    if purged:
        app.logger.info("purged %d old flow(s)", purged)

    enqueued = 0
    if flow_ids:
        try:
            enqueued = enqueue_flow_scoring(flow_ids)
        except Exception as exc:  # pragma: no cover
            app.logger.exception("failed to enqueue flows for scoring: %s", exc)
            try:
                from tasks import score_flow_batch  # type: ignore

                score_flow_batch(flow_ids)
            except Exception:
                app.logger.exception("inline scoring fallback also failed")

    return jsonify({"status": "ok", "ingested": created, "purged": purged, "queued": enqueued})


@app.post("/simulate")
def simulate_attack():
    payload = request.get_json(silent=True) or {}
    attack_type = str(payload.get("type") or "botnet_flood").strip()
    profile = SIMULATION_TYPES.get(attack_type)
    if not profile:
        return jsonify({"error": f"unknown attack type '{attack_type}'"}), 400

    requested_count = payload.get("count")
    default_count = profile["default_count"]
    try:
        count = int(requested_count) if requested_count is not None else default_count
    except (TypeError, ValueError):
        return jsonify({"error": "count must be an integer"}), 400
    count = max(5, min(count, 250))

    flows = generate_attack_flows(attack_type, count)
    for flow in flows:
        db.session.add(flow)
    db.session.flush()

    scores: list[float] = []
    for flow in flows:
        score, label = simulation_detector.predict(db.session, flow)
        scores.append(score)
        db.session.add(
            Prediction(
                flow_id=flow.id,
                score=score,
                label=label,
                created_at=datetime.now(timezone.utc),
            )
        )

    db.session.commit()
    purged = enforce_flow_retention()

    return jsonify(
        {
            "status": "ok",
            "type": attack_type,
            "label": profile["label"],
            "generated": len(flows),
            "max_score": round(max(scores) if scores else 0.0, 3),
            "purged": purged,
        }
    )


# ---------------------------------------------------------------
# Flows endpoint (dashboard)
#  - uses live buffer if present
#  - falls back to demo data if empty
# ---------------------------------------------------------------
@app.get("/flows")
def flows():
    recent = get_recent_flows()
    if recent:
        payload = [flow_to_dict(f) for f in recent]
        return jsonify(payload)

    demo_flows = [
        {
            "ts": "2025-11-17T11:10:00Z",
            "src_ip": "192.168.1.10",
            "dst_ip": "8.8.8.8",
            "proto": "TCP",
            "bytes": 1500,
            "score": 0.12,
        },
        {
            "ts": "2025-11-17T11:10:05Z",
            "src_ip": "10.0.0.5",
            "dst_ip": "172.217.3.110",
            "proto": "TCP",
            "bytes": 4200,
            "score": 0.98,
        },
        {
            "ts": "2025-11-17T11:10:09Z",
            "src_ip": "192.168.1.23",
            "dst_ip": "1.1.1.1",
            "proto": "UDP",
            "bytes": 800,
            "score": 0.45,
        },
    ]
    return jsonify(demo_flows)


@app.get("/anomalous_flows")
def anomalous_flows():
    recent = get_recent_flows()
    if not recent:
        return jsonify([])
    anomalies = [flow for flow in recent if is_anomalous_flow(flow)]
    payload = [flow_to_dict(f) for f in anomalies]
    return jsonify(payload)


# ---------------------------------------------------------------
# Anomaly stats (for now: simple derived stats from buffer or demo)
# ---------------------------------------------------------------
@app.get("/anomalies")
def anomalies():
    data = get_recent_flows()
    if not data:
        # same demo stats as before if nothing ingested yet
        demo_stats = {
            "window": "last 10 min",
            "count": 7,
            "max_score": 0.992,
            "pct_anomalous": 3.1,
        }
        return jsonify(demo_stats)

    scores = [latest_prediction_score(f) for f in data]
    total = len(scores)
    max_score = max(scores) if scores else 0.0
    anomaly_count = sum(1 for s in scores if s > 0.9)
    pct = (anomaly_count / total * 100.0) if total > 0 else 0.0

    stats = {
        "window": "recent buffer",
        "count": anomaly_count,
        "max_score": round(max_score, 3),
        "pct_anomalous": round(pct, 2),
    }
    return jsonify(stats)


@app.post("/block_ip")
def block_ip():
    payload = request.get_json(silent=True) or {}
    ip = str(payload.get("ip") or "").strip()
    if not ip:
        return jsonify({"error": "ip is required"}), 400

    record = BlockedIP.query.filter_by(ip=ip).first()
    now = datetime.now(timezone.utc)
    if record:
        record.active = True
        record.created_at = now
    else:
        db.session.add(BlockedIP(ip=ip, active=True, created_at=now))
    db.session.commit()

    ok, msg = block_ip_os(ip, allow=False)
    return jsonify({"status": "blocked", "ip": ip, "os_action": "ok" if ok else "failed", "detail": msg})


@app.get("/blocked_ips")
def blocked_ips():
    rows = BlockedIP.query.filter_by(active=True).order_by(BlockedIP.created_at.desc()).all()
    payload = [{"ip": row.ip, "created_at": row.created_at.isoformat()} for row in rows]
    return jsonify(payload)


@app.route("/killswitch", methods=["GET", "POST"])
def killswitch():
    if request.method == "POST":
        payload = request.get_json(silent=True) or {}
        enabled = bool(payload.get("enabled"))
        KILL_SWITCH_STATE["enabled"] = enabled
        ensure_killswitch_rules_enabled(enabled)
        return jsonify({"enabled": enabled})
    return jsonify({"enabled": bool(KILL_SWITCH_STATE.get("enabled", False))})


# ---------------------------------------------------------------
# Main Entrypoint (for direct run; Gunicorn ignores this block)
# ---------------------------------------------------------------
if __name__ == "__main__":
    app.run(host="127.0.0.1", port=5000)
